Downloading `prism-dinosiglip-224px+7b from HF Hub
Found Config =>> Loading & Freezing [bold blue]prism-dinosiglip-224px+7b[/] with:
             Vision Backbone =>> [bold]dinosiglip-vit-so-224px[/]
             LLM Backbone    =>> [bold]llama2-7b-pure[/]
             Arch Specifier  =>> [bold]no-align+fused-gelu-mlp[/]
             Checkpoint Path =>> [underline]`./hub/models--TRI-ML--prismatic-vlms/snapshots/a3ba8a19c453a82eaf5a3fb1e699dd9e441f0a12/prism-dinosiglip-224px+7b/checkpoints/latest-checkpoint.pt`[/]
Loading Vision Backbone [bold]dinosiglip-vit-so-224px[/]
02/08 [08:12:42] INFO     | >> Loading pretrained weights from   _builder.py:186
                          Hugging Face hub
                          (timm/vit_large_patch14_reg4_dinov2.lv
                          d142m)
                 INFO     | >>  Safe alternative available for       _hub.py:180
                          'pytorch_model.bin' (as
                          'model.safetensors'). Loading weights
                          using safetensors.
                 INFO     | >> Resized position embedding: (37,  pos_embed.py:55
                          37) to (16, 16).
02/08 [08:12:47] INFO     | >> Loading pretrained weights from   _builder.py:186
                          Hugging Face hub
                          (('timm/ViT-SO400M-14-SigLIP',
                          'open_clip_pytorch_model.bin'))
02/08 [08:12:48] INFO     | >>  Safe alternative available for       _hub.py:180
                          'open_clip_pytorch_model.bin' (as
                          'open_clip_model.safetensors'). Loading
                          weights using safetensors.
Loading Pretrained LLM [bold]llama2-7b-pure[/] via HF Transformers
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.87s/it]
Loading VLM [bold blue]prism-dinosiglip-224px+7b[/] from Checkpoint
Rank 3 has 7541237184 parameters
Rank 3 has 7541237184 parameters
2025-02-08 08:14:00,212 Agent parameters: 7541237184. Trainable: 71385600
2025-02-08 08:14:00,213 Finished setting up policy.
The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.
