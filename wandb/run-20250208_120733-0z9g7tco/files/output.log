Downloading `prism-dinosiglip-224px+7b from HF Hub
Found Config =>> Loading & Freezing [bold blue]prism-dinosiglip-224px+7b[/] with:
             Vision Backbone =>> [bold]dinosiglip-vit-so-224px[/]
             LLM Backbone    =>> [bold]llama2-7b-pure[/]
             Arch Specifier  =>> [bold]no-align+fused-gelu-mlp[/]
             Checkpoint Path =>> [underline]`./hub/models--TRI-ML--prismatic-vlms/snapshots/a3ba8a19c453a82eaf5a3fb1e699dd9e441f0a12/prism-dinosiglip-224px+7b/checkpoints/latest-checkpoint.pt`[/]
Loading Vision Backbone [bold]dinosiglip-vit-so-224px[/]
02/08 [12:07:38] INFO     | >> Loading pretrained weights from   _builder.py:186
                          Hugging Face hub
                          (timm/vit_large_patch14_reg4_dinov2.lv
                          d142m)
                 INFO     | >>  Safe alternative available for       _hub.py:180
                          'pytorch_model.bin' (as
                          'model.safetensors'). Loading weights
                          using safetensors.
                 INFO     | >> Resized position embedding: (37,  pos_embed.py:55
                          37) to (16, 16).
02/08 [12:07:43] INFO     | >> Loading pretrained weights from   _builder.py:186
                          Hugging Face hub
                          (('timm/ViT-SO400M-14-SigLIP',
                          'open_clip_pytorch_model.bin'))
                 INFO     | >>  Safe alternative available for       _hub.py:180
                          'open_clip_pytorch_model.bin' (as
                          'open_clip_model.safetensors'). Loading
                          weights using safetensors.
Loading Pretrained LLM [bold]llama2-7b-pure[/] via HF Transformers
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]
Loading VLM [bold blue]prism-dinosiglip-224px+7b[/] from Checkpoint
Rank 2 has 7541237184 parameters
Rank 2 has 7541237184 parameters
Traceback (most recent call last):
  File "run.py", line 105, in <module>
    main()
  File "run.py", line 55, in main
    run_exp(**vars(args))
  File "run.py", line 97, in run_exp
    trainer.train()
  File "/mnt/data1/users/sgyson10/diffuser_navigator/diffuser_baselines/openvln_trainer_fsdp.py", line 895, in train
    self._initialize_policy(
  File "/mnt/data1/users/sgyson10/diffuser_navigator/diffuser_baselines/openvln_trainer_fsdp.py", line 1150, in _initialize_policy
    self.policy = FSDP(self.policy,cpu_offload=CPUOffload(offload_params=True))
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 487, in __init__
    _init_param_handle_from_module(
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 519, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/torch/distributed/fsdp/_init_utils.py", line 531, in _init_param_handle_from_params
    handle = FlatParamHandle(
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 537, in __init__
    self._init_flat_param_and_metadata(
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 585, in _init_flat_param_and_metadata
    ) = self._validate_tensors_to_flatten(params)
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/torch/distributed/fsdp/flat_param.py", line 731, in _validate_tensors_to_flatten
    raise ValueError(
ValueError: Must flatten tensors with uniform `requires_grad` when `use_orig_params=False`
