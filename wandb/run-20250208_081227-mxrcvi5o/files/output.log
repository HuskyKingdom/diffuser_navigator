02/08 [08:12:37] INFO     | >> [*] Downloading                        load.py:67
                          `prism-dinosiglip-224px+7b from HF Hub
Downloading `prism-dinosiglip-224px+7b from HF Hub
wandb: 500 encountered ({"errors":[{"message":"An internal error occurred. Please contact support.","path":["upsertBucket"]}],"data":{"upsertBucket":null}}), retrying request
02/08 [08:12:39] INFO     | >> [*] Found Config =>> Loading &         load.py:81
                          Freezing prism-dinosiglip-224px+7b with:
                                       Vision Backbone =>>
                          dinosiglip-vit-so-224px
                                       LLM Backbone    =>>
                          llama2-7b-pure
                                       Arch Specifier  =>>
                          no-align+fused-gelu-mlp
                                       Checkpoint Path =>>
                          `./hub/models--TRI-ML--prismatic-vlms/snaps
                          hots/a3ba8a19c453a82eaf5a3fb1e699dd9e441f0a
                          12/prism-dinosiglip-224px+7b/checkpoints/la
                          test-checkpoint.pt`
Found Config =>> Loading & Freezing [bold blue]prism-dinosiglip-224px+7b[/] with:
             Vision Backbone =>> [bold]dinosiglip-vit-so-224px[/]
             LLM Backbone    =>> [bold]llama2-7b-pure[/]
             Arch Specifier  =>> [bold]no-align+fused-gelu-mlp[/]
             Checkpoint Path =>> [underline]`./hub/models--TRI-ML--prismatic-vlms/snapshots/a3ba8a19c453a82eaf5a3fb1e699dd9e441f0a12/prism-dinosiglip-224px+7b/checkpoints/latest-checkpoint.pt`[/]
                 INFO     | >> [*] Loading Vision Backbone            load.py:98
                          dinosiglip-vit-so-224px
Loading Vision Backbone [bold]dinosiglip-vit-so-224px[/]
02/08 [08:12:42] INFO     | >> Loading pretrained weights from   _builder.py:186
                          Hugging Face hub
                          (timm/vit_large_patch14_reg4_dinov2.lv
                          d142m)
                 INFO     | >>  Safe alternative available for       _hub.py:180
                          'pytorch_model.bin' (as
                          'model.safetensors'). Loading weights
                          using safetensors.
                 INFO     | >> Resized position embedding: (37,  pos_embed.py:55
                          37) to (16, 16).
02/08 [08:12:47] INFO     | >> Loading pretrained weights from   _builder.py:186
                          Hugging Face hub
                          (('timm/ViT-SO400M-14-SigLIP',
                          'open_clip_pytorch_model.bin'))
02/08 [08:12:48] INFO     | >>  Safe alternative available for       _hub.py:180
                          'open_clip_pytorch_model.bin' (as
                          'open_clip_model.safetensors'). Loading
                          weights using safetensors.
                 INFO     | >> [*] Loading Pretrained LLM            load.py:107
                          llama2-7b-pure via HF Transformers
Loading Pretrained LLM [bold]llama2-7b-pure[/] via HF Transformers
                 INFO     | >>     |=> Loading llama2 LLM from   base_llm.py:117
                          `meta-llama/Llama-2-7b-hf`
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.11s/it]
02/08 [08:12:59] INFO     | >>     |=> Loading llama2 (Fast)     base_llm.py:147
                          Tokenizer via the AutoTokenizer API
02/08 [08:13:46] INFO     | >> [*] Loading VLM                       load.py:118
                          prism-dinosiglip-224px+7b from Checkpoint
Loading VLM [bold blue]prism-dinosiglip-224px+7b[/] from Checkpoint
02/08 [08:13:55] INFO     | >>     |=> [Frozen]    ðŸ¥¶ =>>       prismatic.py:151
                          Vision Backbone
                          `dinosiglip-vit-so-224px`
                 INFO     | >>     |=> [Frozen]    ðŸ¥¶ =>> LLM   prismatic.py:152
                          Backbone `llama2-7b-pure`
                 INFO     | >>     |=> [TRAINABLE] ðŸ”¥ =>>       prismatic.py:153
                          Projector `no-align+fused-gelu-mlp`
Rank 0 has 7541237184 parameters
Rank 0 has 7541237184 parameters
2025-02-08 08:14:00,211 Agent parameters: 7541237184. Trainable: 71385600
2025-02-08 08:14:00,212 Finished setting up policy.
Epochs:   0%|          | 0/16 [00:00<?, ?it/s] The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.
Batches:   0%|          | 0/25 [00:00<?, ?it/s]         2025-02-08 08:14:48,764 epoch loss: 4.094296226898829  | steps 24 | On Diffuser iter 0, Epoch 0.
Epochs:   6%|â–‹         | 1/16 [00:46<11:35, 46.34s/it]  2025-02-08 08:15:34,820 epoch loss: 1.3668993612130482  | steps 24 | On Diffuser iter 0, Epoch 1.
Not to save.
Epochs:  12%|â–ˆâ–Ž        | 2/16 [01:32<10:45, 46.14s/it]  2025-02-08 08:16:20,758 epoch loss: 1.1141640519102414  | steps 24 | On Diffuser iter 0, Epoch 2.
Not to save.
Epochs:  19%|â–ˆâ–‰        | 3/16 [03:04<13:17, 61.33s/it]  
Not to save.
Traceback (most recent call last):                      
  File "run.py", line 105, in <module>
    main()
  File "run.py", line 55, in main
    run_exp(**vars(args))
  File "run.py", line 97, in run_exp
    trainer.train()
  File "/mnt/data1/users/sgyson10/diffuser_navigator/diffuser_baselines/openvln_trainer.py", line 975, in train
    self.save_checkpoint(
  File "/mnt/data1/users/sgyson10/diffuser_navigator/diffuser_baselines/openvln_trainer.py", line 1130, in save_checkpoint
    "scheduler_state": self.scheduler.state_dict(),
AttributeError: 'OpenVLNTrainer' object has no attribute 'scheduler'
