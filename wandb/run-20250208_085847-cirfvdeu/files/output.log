02/08 [08:58:49] INFO     | >> [*] Downloading                        load.py:67
                          `prism-dinosiglip-224px+7b from HF Hub
Downloading `prism-dinosiglip-224px+7b from HF Hub
02/08 [08:58:52] INFO     | >> [*] Found Config =>> Loading &         load.py:81
                          Freezing prism-dinosiglip-224px+7b with:
                                       Vision Backbone =>>
                          dinosiglip-vit-so-224px
                                       LLM Backbone    =>>
                          llama2-7b-pure
                                       Arch Specifier  =>>
                          no-align+fused-gelu-mlp
                                       Checkpoint Path =>>
                          `./hub/models--TRI-ML--prismatic-vlms/snaps
                          hots/a3ba8a19c453a82eaf5a3fb1e699dd9e441f0a
                          12/prism-dinosiglip-224px+7b/checkpoints/la
                          test-checkpoint.pt`
Found Config =>> Loading & Freezing [bold blue]prism-dinosiglip-224px+7b[/] with:
             Vision Backbone =>> [bold]dinosiglip-vit-so-224px[/]
             LLM Backbone    =>> [bold]llama2-7b-pure[/]
             Arch Specifier  =>> [bold]no-align+fused-gelu-mlp[/]
             Checkpoint Path =>> [underline]`./hub/models--TRI-ML--prismatic-vlms/snapshots/a3ba8a19c453a82eaf5a3fb1e699dd9e441f0a12/prism-dinosiglip-224px+7b/checkpoints/latest-checkpoint.pt`[/]
                 INFO     | >> [*] Loading Vision Backbone            load.py:98
                          dinosiglip-vit-so-224px
Loading Vision Backbone [bold]dinosiglip-vit-so-224px[/]
02/08 [08:58:55] INFO     | >> Loading pretrained weights from   _builder.py:186
                          Hugging Face hub
                          (timm/vit_large_patch14_reg4_dinov2.lv
                          d142m)
                 INFO     | >>  Safe alternative available for       _hub.py:180
                          'pytorch_model.bin' (as
                          'model.safetensors'). Loading weights
                          using safetensors.
                 INFO     | >> Resized position embedding: (37,  pos_embed.py:55
                          37) to (16, 16).
02/08 [08:59:00] INFO     | >> Loading pretrained weights from   _builder.py:186
                          Hugging Face hub
                          (('timm/ViT-SO400M-14-SigLIP',
                          'open_clip_pytorch_model.bin'))
02/08 [08:59:01] INFO     | >>  Safe alternative available for       _hub.py:180
                          'open_clip_pytorch_model.bin' (as
                          'open_clip_model.safetensors'). Loading
                          weights using safetensors.
                 INFO     | >> [*] Loading Pretrained LLM            load.py:107
                          llama2-7b-pure via HF Transformers
Loading Pretrained LLM [bold]llama2-7b-pure[/] via HF Transformers
                 INFO     | >>     |=> Loading llama2 LLM from   base_llm.py:117
                          `meta-llama/Llama-2-7b-hf`
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.97s/it]
02/08 [08:59:12] INFO     | >>     |=> Loading llama2 (Fast)     base_llm.py:147
                          Tokenizer via the AutoTokenizer API
02/08 [09:00:01] INFO     | >> [*] Loading VLM                       load.py:118
                          prism-dinosiglip-224px+7b from Checkpoint
Loading VLM [bold blue]prism-dinosiglip-224px+7b[/] from Checkpoint
02/08 [09:00:09] INFO     | >>     |=> [Frozen]    ðŸ¥¶ =>>       prismatic.py:151
                          Vision Backbone
                          `dinosiglip-vit-so-224px`
                 INFO     | >>     |=> [Frozen]    ðŸ¥¶ =>> LLM   prismatic.py:152
                          Backbone `llama2-7b-pure`
                 INFO     | >>     |=> [TRAINABLE] ðŸ”¥ =>>       prismatic.py:153
                          Projector `no-align+fused-gelu-mlp`
Rank 0 has 7541237184 parameters
Traceback (most recent call last):
  File "run.py", line 105, in <module>
    main()
  File "run.py", line 55, in main
    run_exp(**vars(args))
  File "run.py", line 97, in run_exp
    trainer.train()
  File "/mnt/data1/users/sgyson10/diffuser_navigator/diffuser_baselines/openvln_trainer.py", line 883, in train
    self._initialize_policy(
  File "/mnt/data1/users/sgyson10/diffuser_navigator/diffuser_baselines/openvln_trainer.py", line 1085, in _initialize_policy
    policy = get_peft_model(policy, self.lora_config)
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/peft/mapping.py", line 193, in get_peft_model
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/peft/peft_model.py", line 1609, in __init__
    super().__init__(model, peft_config, adapter_name, **kwargs)
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/peft/peft_model.py", line 171, in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 141, in __init__
    super().__init__(model, config, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 184, in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 435, in inject_adapter
    peft_config = self._prepare_adapter_config(peft_config, model_config)
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 464, in _prepare_adapter_config
    raise ValueError("Please specify `target_modules` in `peft_config`")
ValueError: Please specify `target_modules` in `peft_config`
