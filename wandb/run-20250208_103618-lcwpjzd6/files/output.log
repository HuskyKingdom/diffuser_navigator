Downloading `prism-dinosiglip-224px+7b from HF Hub
Found Config =>> Loading & Freezing [bold blue]prism-dinosiglip-224px+7b[/] with:
             Vision Backbone =>> [bold]dinosiglip-vit-so-224px[/]
             LLM Backbone    =>> [bold]llama2-7b-pure[/]
             Arch Specifier  =>> [bold]no-align+fused-gelu-mlp[/]
             Checkpoint Path =>> [underline]`./hub/models--TRI-ML--prismatic-vlms/snapshots/a3ba8a19c453a82eaf5a3fb1e699dd9e441f0a12/prism-dinosiglip-224px+7b/checkpoints/latest-checkpoint.pt`[/]
Loading Vision Backbone [bold]dinosiglip-vit-so-224px[/]
02/08 [10:36:25] INFO     | >> Loading pretrained weights from   _builder.py:186
                          Hugging Face hub
                          (timm/vit_large_patch14_reg4_dinov2.lv
                          d142m)
                 INFO     | >>  Safe alternative available for       _hub.py:180
                          'pytorch_model.bin' (as
                          'model.safetensors'). Loading weights
                          using safetensors.
                 INFO     | >> Resized position embedding: (37,  pos_embed.py:55
                          37) to (16, 16).
02/08 [10:36:30] INFO     | >> Loading pretrained weights from   _builder.py:186
                          Hugging Face hub
                          (('timm/ViT-SO400M-14-SigLIP',
                          'open_clip_pytorch_model.bin'))
                 INFO     | >>  Safe alternative available for       _hub.py:180
                          'open_clip_pytorch_model.bin' (as
                          'open_clip_model.safetensors'). Loading
                          weights using safetensors.
Loading Pretrained LLM [bold]llama2-7b-pure[/] via HF Transformers
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.71s/it]
Loading VLM [bold blue]prism-dinosiglip-224px+7b[/] from Checkpoint
Rank 2 has 7541237184 parameters
Rank 2 has 7541237184 parameters
2025-02-08 10:37:48,216 Agent parameters: 7541237184. Trainable: 6810325504
2025-02-08 10:37:48,217 Finished setting up policy.
way before Allocated memory: 53.55 GB, Reserved memory: 53.68 GB, Total memory: 79.14 GB, Free memory: 25.46 GB
The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.
before Allocated memory: 67.98 GB, Reserved memory: 68.29 GB, Total memory: 79.14 GB, Free memory: 10.85 GB
after Allocated memory: 67.98 GB, Reserved memory: 68.08 GB, Total memory: 79.14 GB, Free memory: 11.06 GB
Traceback (most recent call last):
  File "run.py", line 105, in <module>
    main()
  File "run.py", line 55, in main
    run_exp(**vars(args))
  File "run.py", line 97, in run_exp
    trainer.train()
  File "/mnt/data1/users/sgyson10/diffuser_navigator/diffuser_baselines/openvln_trainer.py", line 965, in train
    loss = self._update_agent(
  File "/mnt/data1/users/sgyson10/diffuser_navigator/diffuser_baselines/openvln_trainer.py", line 1057, in _update_agent
    loss.backward()
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/users/sgyson10/volatile/miniconda3/envs/openvln/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 2 has a total capacty of 79.14 GiB of which 51.50 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.41 GiB is allocated by PyTorch, and 2.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
